{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Welcome to the project documentation for the electricity demand prediction service! This is my first end-to-end MLOps project and I wanted to document the different parts of the project and not just the code.  </p> <p>The goal of this project is to build an automated MLOps pipeline to:</p> <ul> <li>Fetch data from two APIs, transform it and store in a feature store</li> <li>Re-train a machine learning model (algorithm is yet to be determined) to predict electricity demand given a set of inputs</li> <li>Serve the trained model as a prediction service</li> <li>Monitor the performance of the model and the data and trigger the re-training of the model if required</li> </ul> <p>I will be using Machine Learning Engineering in Action by Ben Wilson Currently to guide me in the process of building an end-to-end pipeline. Currently, I'm working on the experimentation stage.</p>"},{"location":"project_documentation/architecture/","title":"Architecture","text":""},{"location":"project_documentation/architecture/#requirements","title":"Requirements","text":""},{"location":"project_documentation/architecture/#high-level-architecture","title":"High-level architecture","text":""},{"location":"project_documentation/architecture/#data-ingestion-and-feature-store","title":"Data ingestion and feature store","text":"<p>The data ingestion process is very simple for this project as it only requires fetching data from two APIs using the <code>requests</code> package. No real time data will be used for the prediction service, in fact, only the weather data will be used for online inference. Electricity demand data will only be used for training purposes. The data is currently stored locally using parquet files, although it may be stored in the cloud in the future.</p> <p>The data is transformed before being passed to a feature store. A feature store is a centralised repository for storing, managing, and serving machine learning features or attributes used for training predictive models. The developers of Feast, an open-sourced feature store, wrote a great article that explains feature stores in depth.</p> <p>At the start of this project, I came across the idea of features platforms, which build on feature stores and add feature transform pipelines. This functionality can be achieved by combining feature stores with feature transformation pipelines in Spark or Pandas, albeit not that scalable. Since this project will only transform data coming from two sources, a feature store is enough.</p> <p>The architecture of the data ingestion and the feature store is as follows:</p> <p></p> <p>Data ingestion and feature store - Generated using draw.io</p>"},{"location":"project_documentation/architecture/#tools","title":"Tools","text":""},{"location":"project_documentation/architecture/#data-management","title":"Data Management","text":"<ul> <li>Git LFS. Git LFS is the tool currently used for data management since it's very easy to integrate with GitHub and GitHub Actions.</li> <li>Data Version Control. </li> </ul>"},{"location":"project_documentation/architecture/#data-ingestion-and-feature-store_1","title":"Data ingestion and feature store","text":"<ul> <li>Data transformation. The choice hasn't been made yet. Two different alternatives are considered:</li> <li>Pandas</li> <li>Apache beam</li> <li>Feast for feature store. The choice isn't definitive, but Feast is the preferred choice since it's open-sourced</li> </ul>"},{"location":"project_documentation/data/","title":"Data","text":"<p>This project uses two different data sources:</p> <ul> <li>Historic electricity consumption in the UK from the National Grid</li> <li>Weather data from the MET Office.</li> </ul>"},{"location":"project_documentation/data/#electricity-demand-data","title":"Electricity demand data","text":"<p>The historic electricity demand data is collated by National Grid, who operate the electricity grid in the UK. The data spans from 2009 to the current date and it can be accessed using their API. The data is often updated on a daily basis, but the update frequency isn't a problem since this dataset is only used for training and testing of ML models and not for inference. All the data is available and therefore the data can be accessed when needed.  </p> <p>The dataset contains a total of 21 columns such as total electricity demand, renewable energy capacity, interconnector flow, etc ( a description of all the columns can be found here). However, only three columns are used as part of model training:</p> <ul> <li>SETTLEMENT_DATE. The date the historic outturn occurred.</li> <li>SETTLEMENT_PERIOD. The half hourly period for the historic outturn occurred. Settlement period 1 runs from 00:00-00:30.</li> <li>ENGLAND_WALES_DEMAND. England and Wales Demand, as ND above but on an England and Wales basis<ul> <li>ND is described as \"Great Britain generation requirement and is equivalent to the Initial National Demand Outturn (INDO) and National Demand Forecast as published on BM Reports. National Demand is the sum of metered generation, but excludes generation required to meet station load, pump storage pumping and interconnector exports. National Demand is calculated as a sum of generation based on National Grid ESO operational generation metering\"</li> </ul> </li> </ul> <p>Out of those three columns, ENGLAND_WALES_DEMAND is the target, i.e. the value the model will predict, and the other two columns are simply used to assemble a date column that includes both the day and the hour. The date is then used to generated some predictive features such as the day of the week or the week of the year. However, no electricity demand data is used for the prediction service.</p>"},{"location":"project_documentation/data/#weather-data","title":"Weather data","text":"<p>Weather data is collected daily all over the UK by the Met Office. Their API provides a lot of data, but only the hourly site-specific observations are required for training and testing ML models.</p> <p>Hourly site-specific observations include 10 features, such as temperature, wind direction and pressure. However, only two features are used for model training:</p> <ul> <li>Temperature</li> <li>Weather type (see more in Annex A)</li> </ul> <p>Unlike the historic electricity demand data, the hourly site-specific observations are only available for the last 24 hours. Therefore, a script is daily run using Github actions to fetch the daily data and store in a parquet file. The downside of this approach is that historic data isn't available. The earliest the script was run was 26/01/2024, which means that only data from 26/01/2024 will be used to train the model.</p>"},{"location":"project_documentation/data/#weather-data-for-model-prediction","title":"Weather data for model prediction","text":"<p>The weather features mentioned in the previous section, i.e. temperature and weather type, will also be used as part of the predictions service. Met Office provides weather data forecast for the next five days with time steps being daily or every three hours. </p> <p>Since the project is still in the early stages of development, a decision has not been made regarding the data that will be used for the prediction service, but most likely it will use the weather forecast data in intervals of three hours.</p>"},{"location":"project_documentation/data/#annex-a-weather-types","title":"Annex A: Weather types","text":"<p>The original \"weather type\" in the data provided by the Met Office uses numbers to encode different weather types as follows:</p> Code Description 0 Clear night 1 Sunny day 2 Partly cloudy (night) 3 Partly cloudy (day) 4 Not used 5 Mist 6 Fog 7 Cloudy 8 Overcast 9 Light rain shower (night) 10 Light rain shower (day) 11 Drizzle 12 Light rain 13 Heavy rain shower (night) 14 Heavy rain shower (day) 15 Heavy rain 16 Sleet shower (night) 17 Sleet shower (day) 18 Sleet 19 Hail shower (night) 20 Hail shower (day) 21 Hail 22 Light snow shower (night) 23 Light snow shower (day) 24 Light snow 25 Heavy snow shower (night) 26 Heavy snow shower (day) 27 Heavy snow 28 Thunder shower (night) 29 Thunder shower (day) 30 Thunder? <p>As can be seen, some of the values are the same for night and day. In order to reduce the complexity of this feature, the data encoding was modified to the following:</p> Code Description 0 Clear night 1 Sunny day 2 Partly cloudy 3 Not used 4 Mist 5 Fog 6 Cloudy 7 Overcast 8 Light rain 9 Drizzle 10 Heavy rain 11 Sleet 12 Hail 13 Light snow 14 Heavy snow 15 Thunder"},{"location":"project_documentation/experimentation/","title":"Experimentation","text":""},{"location":"project_documentation/experimentation/#introduction","title":"Introduction","text":"<p>For this stage, I will use chapters 5 and 6 from the book Machine Learning Engineering in Action by Ben Wilson as reference material. Chapter 5 focuses on planning and researching and ML project and chapter 6 describes testing and evaluation of different ML models. Ben uses a examples in all the chapters and the example in this section of the book is a time series forecasting model and therefore what he describes is quite relevant for me.</p> <p>Prior to describing my approach to the experimentation phase, it's worth noting in a previous project available on Kaggle I already did a lof of the work that a data scientist/ML engineer would do during the experimentation stage, such as data analysis and model benchmarking. </p> <p>I created a Jupyter notebook that covers the step I took for model experimentation. This page summarises the key findings in the notebook.</p>"},{"location":"project_documentation/experimentation/#planning","title":"Planning","text":"<p>The research I conducted for my previous project on electricity demand forecasting was very helpful for this project. Since I worked on that project (mid 2022), there haven't been groundbreaking new techniques for time series forecasting. As part of that project, I used:</p> <ul> <li>SARIMA models. Despite trying multiple combinations of hyperparameters, each of them leading to a more complex and bigger model, SARIMA couldn't capture the complexity of the data (see next section)</li> <li>XGBoost. XGBoost yielded the best results, but as proven by in this Kaggle notebook by Carl McBride Ellis a very respected member of the Kaggle community, gradient boosted frameworks, including XGBoost, cannot extrapolate and their performance decreases when the forecasting on unseen data.</li> <li>Linear Boost Regression - part of Linear Trees. I came across this python packages in a Kaggle notebook also by Carl McBride Ellis. It offered the most promising results, but it was fairly slow to train on CPU (around 10 minutes per run for a simple model).</li> <li>Prophet. The results were comparable to that of the other models, but I have since encountered some articles that discourage the use of Prophet.</li> <li>LSTM and Deep LSTM (using Keras). LSTM networks were the most accurate model, but also the one that took the longest to train despite having GPU acceleration enabled in Kaggle. Therefore, given the expensive computational requirements, it won't be used for this project. </li> </ul> <p>The results I got (as of version 16) are as follows:</p> Metric XGBoost - Simple XGBoost - CV &amp; GS Linear Boost Prophet - Simple Prophet - Holiday Prophet - CV &amp; GS LSTM Deep LSTM MAPE 8.91 7.34 8.63 9.37 9.36 9.36 7.39 7.22 RMSE 3383.98 2670.52 3112.08 3262.10 3243.37 3241.11 2708.95 2594.04 <p>It is worth noting that for Kaggle project I use the variable Total System Demand (TSD), which includes the demand from England and Wales, plus Scotland and other values. Therefore, the magnitude of that variable is larger and the RMSE (Root Mean Square Errors) values in that table will be larger than those I obtained for this project.</p>"},{"location":"project_documentation/experimentation/#analysis","title":"Analysis","text":"<p>The first step of the analysis stage is to visualise the data: </p> <p>The above image shows the electricity demand in England and Wales, the one day rolling average and the one week rolling 2 sigma curves.</p> <p>Something that stands out in the previous images is the parts of the plot where there's a straight line connecting sampled from different days. That corresponds to missing values. In order to keep the daily seasonality, which can be seen in the next plot, I removed the data for those days where there is at least one missing sample. In order to observe the daily pattern, one can zoom in and focus on just one week: </p> <p>In order to keep this section brief, the reader is encouraged to check the Jupyter notebook to find out more about the decomposition into trend, seasonality and residuals. Since ARIMA model won't be used, these parameters aren't that relevant.</p> <p>Also, the Kaggle notebook helped me gain more insight about the data since I could use the electricity demand from 2009 to 2024 (in case you wonder why I'm not using it for this project, it's because the weather data is only available from 26/01/2024):</p> <ul> <li>There's weekly seasonality and yearly seasonality in the data</li> <li>Bank holidays affect the energy consumption</li> <li>Electricity consumption is lower during summer</li> <li>Electricity consumption is trending down. In 2009, the average electricity consumption (for TSD, not just England and Wales) was around 38,000 MW per hour, whereas in 2023 the average electricity consumption was 28,000 MW per hour</li> </ul>"},{"location":"project_documentation/experimentation/#prototyping","title":"Prototyping","text":"<p>Before talking about the models, it is necessary to define the metrics that will be used to assess the performance:</p> <ul> <li>R-square</li> <li>Mean Square Error (MSE)</li> <li>Root Mean Square Error (RMSE)</li> <li>Mean Absolute Error (MAE)</li> <li>Mean Absolute Percentage Error (MAPE)</li> </ul> <p>In the Planning section section I mentioned the models I used in my previous project on electricity demand forecasting. For this project, I considered the following models:</p> <ul> <li>XGBoost</li> <li>Linear Tree</li> <li>Linear Boosting</li> </ul> <p>All of these algorithms meet some of the requirements of the prediction service: they're fast to train on CPU.</p> <p>Beyond finding what the best performing model, I wanted to test whether adding weather data led to more accurate predictions. In my original project, I only used the past data and features such as the day of the week and month of the year to train the model. Adding weather data leads to a more complex infrastructure, but the extra complexity is worth it if it leads to more accurate predictions.</p>"},{"location":"project_documentation/experimentation/#evaluation","title":"Evaluation","text":"<p>The goal of running the experiments was to:</p> <ul> <li>Find out what model yields the best results with minimal hyperparameter tunning (proper hyperparameter tunning will be performed at a later stage)</li> <li>Find out if adding weather data improves the predictions</li> </ul> <p>The results are summarised by the following table:</p> Model MAE MAPE MSE RMSE Explained Var R2 xgb_simple 2292.5 11.6 7892710.0 2809.4 0.7 0.3 xgb_advanced 2302.6 11.7 7976893.0 2824.3 0.8 0.3 linear_boost_simple 2295.6 11.3 8080621.0 2842.6 0.4 0.3 linear_boost_advanced 3291.8 16.6 15724100.0 3965.4 0.2 -0.3 linear_trees_simple 1563.8 7.5 4059538.0 2014.8 0.7 0.7 linear_trees_advanced 1551.7 7.5 4014264.0 2003.6 0.7 0.7 xgb_simple - no weather 2992.0 15.1 12084520.0 3476.3 0.7 -0.0 xgb_advanced - no weather 3377.8 16.9 14913750.0 3861.8 0.7 -0.3 linear_boost_simple - no weather 2107.2 10.6 7109882.0 2666.4 0.5 0.4 linear_boost_advanced - no weather 4049.6 20.7 57325420.0 7571.4 -3.3 -3.9 <p>Note that the difference between simple and advanced models is simply increasing the values of a few hyperparameters. Proper hyperparameter tuning will be performed as part of the MVP.</p> <p>The above dataframe, which is used as a summary table, shows that:</p> <ul> <li>Linear trees are the best performing model, both in its simple and advanced versions. An added bonus is that is the second fastest model to train after XGBoost.</li> <li>Using weather data leads to be better results</li> </ul> <p>The comparison of the linear trees model with the test data is as follows: </p> <p>Before, wrapping up model experimentation. I would like to address some of the questions that Ben Wilson includes at the end of chapter 6 (the rest of the questions aren't that relevant for this project as this project isn't meant to create a proprietary prediction service):</p> <ul> <li> <p>How often does this need to run? (The question actually refers to training and not inference)</p> <ul> <li>The model will be re-trained daily until a stable model is found. After that, the model will only be re-trained when the performance drops beyond a given threshold</li> </ul> </li> <li> <p>Where is the data for this right now?</p> <ul> <li>Data is available right now. Electricity demand data is available at any time and the weather data is fetched, updated and stored daily.</li> </ul> </li> <li> <p>Where are the forecast going to be stored?</p> <ul> <li>The model will be accessed through an API and web app will be available to visualise some dashboards. The tool to store the forecasts hasn't been decided yet, but it needs to fulfill these needs.</li> </ul> </li> <li> <p>Where is this going to run for training?</p> <ul> <li>Once past the MVP stage, the project will be deployed to a server where the training will be run.</li> </ul> </li> <li> <p>Where is the inference going to run?</p> <ul> <li>Same as above, in a server</li> </ul> </li> <li> <p>How are we going to get the predictions to the end users?</p> <ul> <li>Via API</li> </ul> </li> </ul>"},{"location":"project_documentation/ideas/","title":"Ideas","text":""},{"location":"project_documentation/ideas/#how-to-implement-the-prediction","title":"How to implement the prediction","text":"<p>In previous analysis, I've used features such as hour of the day and day of the year to predict the electricity demand. These features are great because the strong seasonalities in electricity demand help make decent predictions, but it isn't enough to make accurate predictions. The goal of this project is to add some of the most important features when it comes to electricity demand: weather data and weather forecasts.</p> <p>It looks simple in theory, just add values like the hourly temperature or the overall weather condition, e.g. sunny or cloudy. In practice, it's more complicated since the data from the National Grid is at national level. The temperature, most likely, won't be the same in London and Manchester, and the same can be said about weather condition.</p> <p>To address this issue, some ideas are suggested:</p> <ul> <li>Let's predict Wales and England demand instead of National demand or TSD</li> <li>Let's use an weighed-average temperature based on population of the area, e.g. if London has 10% of the population, then london_temp*0.1</li> <li>Try to use an \"average\" climate condition: e.g. cloudy, sunny... It could be encoded as a number, but it wouldn't make much sense in real life</li> </ul> <p>The goal is to encode the weather conditions as a single value for the entire territory instead of different values for different locations. This would increase the number of features very quickly, leading to a more complex model, as well as the possibility of adding non-informative features.</p>"},{"location":"project_documentation/ideas/#data-management","title":"Data management","text":"<p>The project is in the early stages and optimal data management strategy is yet to be determined. In its final form, the project will use two API services to pull information periodically, but for now, the data from both sources will be stored in parquet files. The National Grid makes the historic data available and it isn't necessary to pull data every day (currently, it's done weekly). However, it seems that the data from the Met Office is only available for the last 24 hours. Therefore, it needs to be fetched daily. </p> <p>To do so, different options are considered: * Keep track of the data using Git LFS, create a branch for data updates and merge it with develop (or master) periodically. This is the preferred option. * Similar to the previous option, but pulling the data directly into the workflow. Since daily data updates are required, it may make the develop branch and other branches quite messy * Reference the data using Git LFS. Specific versions or commits of the data can be directly referenced in the code. * use a different tool such as Data Version Control.</p> <p>The data used in the project is tabular for both sources and it will stored in parquet files for now.  </p>"},{"location":"project_documentation/overview/","title":"Overview","text":"<p>Welcome to the project documentation for the electricity demand prediction service! This is my first end-to-end MLOps project and I wanted to document the different parts of the project and not just the code.  </p> <p>Initially, I wanted to implement the fancy features from MLOps pipelines such as feature stores, CI pipelines or a prediction service before having carried out the essential steps such as deciding which model I will be using. That's why I'm using the book Machine Learning Engineering in Action by Ben Wilson to guide my project planning and implementation process.</p> <p>The goal of this project is to build an automated MLOps pipeline to:</p> <ul> <li>Fetch data from two APIs, transform it and store in a feature store</li> <li>Re-train a machine learning model (algorithm is yet to be determined) to predict electricity demand given a set of inputs</li> <li>Serve the trained model as a prediction service</li> <li>Monitor the performance of the model and the data and trigger the re-training of the model if required</li> </ul> <p>These ideas are captured by the following diagram: </p> <p>Automated MLOps pipeline. Source: Google MLOps</p> <p>The above diagram shows a fully automated MLOps pipeline and that's the ultimate goal, but the pipeline will be built iteratively and the first iterations won't include as many features</p>"},{"location":"project_documentation/overview/#previous-work-and-project-motivation","title":"Previous work and project motivation","text":"<p>This project is the continuation of a notebook a created on time series analysis and prediction as part of an ML course. The notebook, available on Kaggle, uses past electricity demand data and other features, such as time of the day and day of the year, to analyse the performance of different ML models and make predictions on the hold-out set. </p> <p>That project helped me get a really good insight on the dataset, such as the different seasonal patterns, and the different models use, e.g. SARIMA models couldn't capture the complexity of the data and therefore won't be considered as possible ML models for this project. However, there were two questions I didn't answer as part of that project (not that it was meant to be an introductory project on time series analysis):</p> <ul> <li>What if I added other features such as temperature and weather conditions? Would it improve the accuracy of the predictions?</li> <li>After building an ML model that can make predictions of future electricity demand, can I create a dashboard to visualise the model performance and serve the predictions using an API?</li> </ul> <p>The National Grid, the energy company that manages the electricity grid in the UK and whose data I use, has its own prediction service. The goal of this project isn't to create a prediction service that can compete with that of the National Grid, but to build on previous work and challenge myself to build and end-to-end MLOps platform. </p>"}]}